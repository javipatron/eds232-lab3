---
title: "Lab 3 Demo"
author: "Javier Patrón"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(skimr)
library(glmnet)
```

## Data Wrangling and Exploration
```{r data}
#load and inspect the data
ames <- AmesHousing::make_ames()

```

##Train a model
```{r intial_split}
# Stratified sampling with the rsample package
set.seed(123) #set a seed for reproducibility

split <- initial_split(data = ames , prop = .75, strata = "Sale_Price")
split

ames_train <- training(split)
ames_test  <- testing(split)

skim(ames_train)

```

```{r model_data}
#Create training feature matrices using model.matrix() (auto encoding of categorical variables)

X <- model.matrix(data = ames_train, Sale_Price ~ . ) [,-1]

# transform y with log() transformation
Y <- log(ames_train$Sale_Price)

```

```{r glmnet}
#fit a ridge model, passing X,Y,alpha to glmnet()
ridge <- glmnet(x = X,
                y = Y,
                alpha = 0.5)

#plot() the glmnet model object
  plot(ridge, xvar = "lambda")
  
```

```{r}
# lambdas applied to penalty parameter.  Examine the first few
ridge$lambda |> 
  head()

# small lambda results in large coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 100] #taking out the most important coefficents

# When lambda is big it decreases the coefficients, when lambda is small it increases the coefficents.

# what about for small coefficients?
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"), 1]
  
```
How much improvement to our loss function as lambda changes?


##Tuning
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()
ridge <- cv.glmnet(
  x= X,
  y = Y,
  alpha = 0
)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x= X,
  y = Y,
  alpha = 1
)

# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```

10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard deviation"?

In both models we see a slight improvement in the MSE as our penalty log(λ) gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

Let's examine the important parameter values apparent in the plots.
```{r}
# Ridge model
# minimum MSE
summary(ridge)

# lambda for this min MSE

# lambda for this MSE


# Lasso model
min(lasso$cvm)       # minimum MSE

lasso$lambda.min     # lambda for this min MSE


# 1-SE rule
lasso$lambda.1se  # lambda for this MSE

# No. of coef | 1-SE MSE

```

```{r}
# Ridge model
ridge_min <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Lasso model
lasso_min <- glmnet(
  x = X,
  y = Y,
  alpha = 1
)

par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```

*Insights: v = log(ridge$lambda.min) gives you the min error. Which is the one you are looking for. As we start penalaixing we start improving the overfitting. If the graph start decreasing it means that you could have a lot of overfitting.*

```{r }
skim(ames_train)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
